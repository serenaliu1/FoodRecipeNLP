{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f719b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/elvirabernadet/opt/anaconda3/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/elvirabernadet/opt/anaconda3/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/elvirabernadet/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/elvirabernadet/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcb9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a559dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.food.com\"\n",
    "\n",
    "robots = request.urlopen(URL+\"/robots.txt\").read().decode('utf8')\n",
    "robots = robots.split(\"\\n\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a19402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disallowed = []\n",
    "for line in robots.split(\"\\n\"):\n",
    "    m = re.match(r'^Disallow: (.*)$',line.strip())\n",
    "    if m:\n",
    "        this_url = m.group(1)\n",
    "        # convert the disallowed URLs to regular expressions\n",
    "        this_url = re.sub(r'\\*',\".*\",this_url)\n",
    "        this_url = re.sub(r'\\?',\"\\?\",this_url)\n",
    "        this_url = re.sub(r'\\/$',\"\",this_url)\n",
    "        if (not this_url.startswith(\"http\")): this_url = URL+this_url \n",
    "        disallowed.append(this_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37774822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the main page of Food.com\n",
    "if (not URL in disallowed) and (not \"/\" in disallowed):\n",
    "    food = request.urlopen(URL).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc828422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve url for each cuisine main page\n",
    "\n",
    "url_set = dict()\n",
    "\n",
    "cuisine = ('mexican', 'italian', 'indian', 'thai', 'korean', 'french', 'latin-american', 'chinese', 'japanese', 'spanish')\n",
    "# use Beautiful Soup to find all <a> tags in the page\n",
    "for atag in BeautifulSoup(food,\"html.parser\").find_all('a'):\n",
    "    href = atag.get('href')\n",
    "    if href==None: continue\n",
    "\n",
    "    # Add this hypertext reference to the list, if it's allowed\n",
    "    allowed = 1\n",
    "    for disallow in disallowed:\n",
    "        if re.match(r'^'+disallow+'$',href): \n",
    "            allowed = 0\n",
    "    # preserve only meaningful web pages of news\n",
    "    # if allowed and re.match(r'^' + URL, href): \n",
    "    \n",
    "    for c in cuisine:\n",
    "        # some of the URL are using -food pattern meanwhile the other ones are using -recipe\n",
    "        if allowed and c+'-food' in href:\n",
    "            url_set[c]=URL + href\n",
    "        elif allowed and c+'-recipe' in href:\n",
    "            url_set[c]=URL + href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b65e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve content of each main page of the cuisine\n",
    "\n",
    "link_contents = {}\n",
    "\n",
    "for i in cuisine:\n",
    "    if (not URL in disallowed) and (not \"/\" in disallowed):\n",
    "        link_contents[i] = request.urlopen(url_set[i]).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed90608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve links for recipe inside each cuisine\n",
    "\n",
    "links = {}\n",
    "\n",
    "for i in cuisine:\n",
    "    recipes = []\n",
    "    for atag in BeautifulSoup(link_contents[i],\"html.parser\").find_all('a'):\n",
    "        href = atag.get('href')\n",
    "        if href==None: continue\n",
    "            \n",
    "        allowed = 1\n",
    "        for disallow in disallowed:\n",
    "            if re.match(r'^'+disallow+'$',href): \n",
    "                allowed = 0\n",
    "            \n",
    "        if allowed and 'www.food.com/recipe/' in href:\n",
    "            recipes.append(href)\n",
    "            \n",
    "    links[i] = recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c66f2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve full recipes\n",
    "# this may take some time to run\n",
    "\n",
    "allrecipes = []\n",
    "\n",
    "def crawl_recipe(c):\n",
    "    for i in range (0,len(links[c])):\n",
    "        recipe=[]\n",
    "        \n",
    "        # reading the content of the web page\n",
    "        content = request.urlopen(links[c][i]).read()\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        \n",
    "        # retrieve the dish title\n",
    "        title = soup.find('div', class_= 'layout__item title svelte-mq22ro').h1.text\n",
    "        recipe.append(title)\n",
    "        \n",
    "        # retrieve the list of ingredients\n",
    "        start = 'window.mdManager.setParameter(\"ingredients\", \"'\n",
    "        end = '\");window.mdManager.setParameter(\"keywordids\",'\n",
    "        results = str(soup.find_all('script'))\n",
    "        ingredients = (results.split(start))[1].split(end)[0]\n",
    "        recipe.append(ingredients)\n",
    "        \n",
    "        # input the cuisine name as identifier\n",
    "        recipe.append(c)\n",
    "        # input the recipe to the final var\n",
    "        allrecipes.append(recipe)\n",
    "\n",
    "        \n",
    "# crawl each cuisine\n",
    "for item in cuisine:\n",
    "    crawl_recipe(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9936d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export final result to csv\n",
    "\n",
    "df = pd.DataFrame(allrecipes)\n",
    "headerList = ['Name', 'Ingredients', 'Cuisine']\n",
    "df.to_csv('all_recipes.csv', header=headerList,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
